{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T15:54:50.173957Z",
     "iopub.status.busy": "2025-11-23T15:54:50.173753Z",
     "iopub.status.idle": "2025-11-23T15:54:51.774863Z",
     "shell.execute_reply": "2025-11-23T15:54:51.774032Z",
     "shell.execute_reply.started": "2025-11-23T15:54:50.173941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T04:02:09.832801Z",
     "iopub.status.busy": "2025-11-25T04:02:09.832535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries if running in Colab or a new environment\n",
    "# Install SWIG and build tools\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y swig build-essential\n",
    "\n",
    "# Now install gymnasium with box2d\n",
    "!pip install gymnasium[box2d] torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, ClipAction, NormalizeObservation, TransformObservation, NormalizeReward, TransformReward, RecordEpisodeStatistics\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import base64\n",
    "import pickle\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    ENV_NAME = 'BipedalWalker-v3'\n",
    "    SEED = 9\n",
    "    \n",
    "    # Training Settings\n",
    "    NUMBER_OF_STEPS = 1500    # Total training iterations\n",
    "    BATCH_SIZE = 2048         # Steps per iteration\n",
    "    MINIBATCH_SIZE = 64       \n",
    "    EPOCHS = 10               \n",
    "    \n",
    "    # Hyperparameters\n",
    "    LR_POLICY = 3e-4\n",
    "    LR_CRITIC = 4e-4\n",
    "    GAMMA = 0.99\n",
    "    LAMBDA = 0.95             \n",
    "    CLIP_EPS = 0.2\n",
    "    ENTROPY_COEF = 0.001      \n",
    "    MAX_GRAD_NORM = 0.5\n",
    "    \n",
    "    # Logging & Saving\n",
    "    SAVE_DIR = \"./saved_models\"\n",
    "    VIDEO_DIR = \"./videos\"\n",
    "    CHECKPOINT_FREQ = 100     # Save model every 100 iters\n",
    "    VIDEO_FREQ = 50           # Record video every 50 iters\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. NETWORKS\n",
    "# ==========================================\n",
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(PolicyNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.LayerNorm(256),       \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),       \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_shape)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(output_shape))\n",
    "\n",
    "    def forward(self, x, action=None):\n",
    "        raw_output = self.net(x)\n",
    "        mean = torch.tanh(raw_output) \n",
    "        std = torch.exp(self.log_std).clamp(min=1e-3, max=1.0)\n",
    "        dist = Normal(mean, std)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        return action, dist.log_prob(action).sum(dim=-1), dist.entropy().sum(dim=-1)\n",
    "\n",
    "class CriticNN(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CriticNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PPO AGENT\n",
    "# ==========================================\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, act_dim):\n",
    "        self.policy = PolicyNN(state_dim, act_dim).to(Config.DEVICE)\n",
    "        self.critic = CriticNN(state_dim).to(Config.DEVICE)\n",
    "        self.opt_p = optim.Adam(self.policy.parameters(), lr=Config.LR_POLICY)\n",
    "        self.opt_c = optim.Adam(self.critic.parameters(), lr=Config.LR_CRITIC)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            s = torch.FloatTensor(state).to(Config.DEVICE)\n",
    "            a, lp, _ = self.policy(s)\n",
    "        self.policy.train()\n",
    "        return a.cpu().numpy(), lp.cpu().numpy()\n",
    "\n",
    "    def get_deterministic_action(self, state):\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            s = torch.FloatTensor(state).to(Config.DEVICE)\n",
    "            raw_output = self.policy.net(s)\n",
    "            action = torch.tanh(raw_output)\n",
    "        self.policy.train()\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def update(self, memory):\n",
    "        states = torch.FloatTensor(np.array(memory['states'])).to(Config.DEVICE)\n",
    "        actions = torch.FloatTensor(np.array(memory['actions'])).to(Config.DEVICE)\n",
    "        old_log_probs = torch.FloatTensor(np.array(memory['log_probs'])).to(Config.DEVICE)\n",
    "        rewards = memory['rewards']\n",
    "        dones = memory['dones']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states).squeeze()\n",
    "            next_val = 0 \n",
    "            advantages = torch.zeros_like(values)\n",
    "            last_gae = 0\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                if dones[t]: last_gae = 0; next_val = 0\n",
    "                delta = rewards[t] + Config.GAMMA * next_val * (1-dones[t]) - values[t]\n",
    "                advantages[t] = last_gae = delta + Config.GAMMA * Config.LAMBDA * last_gae * (1-dones[t])\n",
    "                next_val = values[t]\n",
    "            returns = advantages + values\n",
    "\n",
    "        p_losses, c_losses = [], []\n",
    "        for _ in range(Config.EPOCHS):\n",
    "            indices = torch.randperm(len(states))\n",
    "            for i in range(0, len(states), Config.MINIBATCH_SIZE):\n",
    "                idx = indices[i:i+Config.MINIBATCH_SIZE]\n",
    "                _, new_lp, entropy = self.policy(states[idx], actions[idx])\n",
    "                v_pred = self.critic(states[idx]).squeeze()\n",
    "                \n",
    "                ratio = torch.exp(new_lp - old_log_probs[idx])\n",
    "                adv = (advantages[idx] - advantages[idx].mean()) / (advantages[idx].std() + 1e-8)\n",
    "                \n",
    "                loss_p = -torch.min(ratio*adv, torch.clamp(ratio, 1-Config.CLIP_EPS, 1+Config.CLIP_EPS)*adv).mean()\n",
    "                loss_p -= Config.ENTROPY_COEF * entropy.mean()\n",
    "                loss_c = 0.5 * (returns[idx] - v_pred).pow(2).mean()\n",
    "                \n",
    "                self.opt_p.zero_grad(); loss_p.backward(); nn.utils.clip_grad_norm_(self.policy.parameters(), Config.MAX_GRAD_NORM); self.opt_p.step()\n",
    "                self.opt_c.zero_grad(); loss_c.backward(); nn.utils.clip_grad_norm_(self.critic.parameters(), Config.MAX_GRAD_NORM); self.opt_c.step()\n",
    "                p_losses.append(loss_p.item()); c_losses.append(loss_c.item())\n",
    "                \n",
    "        return np.mean(p_losses), np.mean(c_losses)\n",
    "\n",
    "    def save(self, filename):\n",
    "        path = os.path.join(Config.SAVE_DIR, filename)\n",
    "        torch.save({\n",
    "            'policy': self.policy.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "# ==========================================\n",
    "# 4. HELPERS\n",
    "# ==========================================\n",
    "def make_env():\n",
    "    env = gym.make(Config.ENV_NAME)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    env = ClipAction(env)\n",
    "    env = NormalizeObservation(env)\n",
    "    env = TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    env = NormalizeReward(env); env = TransformReward(env, lambda rew: np.clip(rew, -10, 10))\n",
    "    return env\n",
    "\n",
    "def save_normalization_stats(env, filename):\n",
    "    \"\"\"Saves the running mean and variance from the NormalizeObservation wrapper.\"\"\"\n",
    "    path = os.path.join(Config.SAVE_DIR, filename)\n",
    "    try:\n",
    "        obs_rms = env.get_wrapper_attr('obs_rms')\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(obs_rms, f)\n",
    "    except AttributeError:\n",
    "        print(\"Warning: Could not find obs_rms in env wrappers. Stats not saved.\")\n",
    "\n",
    "def record_checkpoint(agent, n_step, prefix=\"PPO\"):\n",
    "    video_folder = f'./videos/{prefix}/step_{n_step}'\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "    \n",
    "    env = gym.make(Config.ENV_NAME, render_mode='rgb_array')\n",
    "    env = RecordVideo(env, video_folder, name_prefix=f\"{prefix}_step_{n_step}\", disable_logger=True)\n",
    "    \n",
    "    env = ClipAction(env); env = NormalizeObservation(env)\n",
    "    env = TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    \n",
    "    state, _ = env.reset(seed=Config.SEED + n_step)\n",
    "    done = False\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).to(Config.DEVICE)\n",
    "            raw_output = agent.policy.net(state_t)\n",
    "            action = torch.tanh(raw_output).cpu().numpy()\n",
    "        state, _, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "    env.close()\n",
    "    print(f\"Recorded video: {video_folder}/{prefix}_step_{n_step}-episode-0.mp4\")\n",
    "\n",
    "def plot_training_results(history):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'PPO Training Results: {Config.ENV_NAME}', fontsize=16)\n",
    "    \n",
    "    def moving_average(data, w=50):\n",
    "        if len(data) < w: return data\n",
    "        d = np.array(data).flatten()\n",
    "        return np.convolve(d, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    # 1. Rewards\n",
    "    axs[0,0].plot(history[\"rewards\"], alpha=0.3, color='blue', label='Raw')\n",
    "    axs[0,0].plot(moving_average(history[\"rewards\"]), color='blue', linewidth=2, label='Avg (50)')\n",
    "    axs[0,0].set_title('Episode Rewards'); axs[0,0].legend(); axs[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Distances (approximated by hull position if available, or just episode lengths as proxy)\n",
    "    # Re-using lengths for visualization if distance tracking fails\n",
    "    axs[0,1].plot(history[\"lengths\"], alpha=0.3, color='green', label='Raw')\n",
    "    axs[0,1].plot(moving_average(history[\"lengths\"]), color='green', linewidth=2, label='Avg (50)')\n",
    "    axs[0,1].set_title('Episode Lengths'); axs[0,1].legend(); axs[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Policy Loss\n",
    "    axs[1,0].plot(history[\"p_loss\"], color='purple')\n",
    "    axs[1,0].set_title('Policy Loss'); axs[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Critic Loss\n",
    "    axs[1,1].plot(history[\"c_loss\"], color='red')\n",
    "    axs[1,1].set_title('Critic Loss'); axs[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN TRAINING LOOP\n",
    "# ==========================================\n",
    "def train():\n",
    "    print(f\"\\n{'='*30}\\nStarting PPO Training\\n{'='*30}\")\n",
    "    \n",
    "    env = make_env()\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    agent = PPOAgent(state_dim, act_dim)\n",
    "    \n",
    "    state, _ = env.reset(seed=Config.SEED)\n",
    "    \n",
    "    # Track detailed metrics\n",
    "    history = {\n",
    "        \"rewards\": [], \n",
    "        \"lengths\": [],\n",
    "        \"distances\": [],\n",
    "        \"p_loss\": [], \n",
    "        \"c_loss\": []\n",
    "    }\n",
    "    best_avg_reward = -float('inf')\n",
    "    \n",
    "    # Record Initial Video\n",
    "    record_checkpoint(agent, 0, prefix=\"PPO_Init\")\n",
    "\n",
    "    for step in range(1, Config.NUMBER_OF_STEPS + 1):\n",
    "        memory = {'states':[], 'actions':[], 'log_probs':[], 'rewards':[], 'dones':[]}\n",
    "        \n",
    "        # Collection Phase\n",
    "        for _ in range(Config.BATCH_SIZE):\n",
    "            action, lp = agent.get_action(state)\n",
    "            next_state, reward, term, trunc, info = env.step(action)\n",
    "            done = term or trunc\n",
    "            \n",
    "            memory['states'].append(state); memory['actions'].append(action); memory['log_probs'].append(lp)\n",
    "            memory['rewards'].append(reward); memory['dones'].append(done)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if \"episode\" in info:\n",
    "                    # Extract scalar values safely\n",
    "                    ep_r = info['episode']['r']\n",
    "                    if isinstance(ep_r, (np.ndarray, np.generic)): ep_r = ep_r.item()\n",
    "                    history[\"rewards\"].append(ep_r)\n",
    "                    \n",
    "                    ep_l = info['episode']['l']\n",
    "                    if isinstance(ep_l, (np.ndarray, np.generic)): ep_l = ep_l.item()\n",
    "                    history[\"lengths\"].append(ep_l)\n",
    "                    \n",
    "                    try:\n",
    "                        dist = env.unwrapped.hull.position[0]\n",
    "                        history[\"distances\"].append(dist)\n",
    "                    except:\n",
    "                        history[\"distances\"].append(0)\n",
    "                \n",
    "                state, _ = env.reset()\n",
    "\n",
    "        # Update\n",
    "        p_loss, c_loss = agent.update(memory)\n",
    "        history[\"p_loss\"].append(p_loss)\n",
    "        history[\"c_loss\"].append(c_loss)\n",
    "\n",
    "        # Logging\n",
    "        if step % 10 == 0:\n",
    "            avg_rew = np.mean(history[\"rewards\"][-50:]) if history[\"rewards\"] else 0.0\n",
    "            avg_dist = np.mean(history[\"distances\"][-50:]) if history[\"distances\"] else 0.0\n",
    "            print(f\"Step {step}/{Config.NUMBER_OF_STEPS} | Reward: {avg_rew:.2f} | Dist: {avg_dist:.2f} | P-Loss: {p_loss:.4f} | C-Loss: {c_loss:.4f}\")\n",
    "\n",
    "        # Checkpoints & Video\n",
    "        if step % Config.VIDEO_FREQ == 0: \n",
    "            record_checkpoint(agent, step, prefix=\"PPO\")\n",
    "        \n",
    "        if step % Config.CHECKPOINT_FREQ == 0:\n",
    "            agent.save(f\"checkpoint_{step}.pth\")\n",
    "            save_normalization_stats(env, f\"checkpoint_{step}_stats.pkl\")\n",
    "        \n",
    "        # Best Model Save\n",
    "        if len(history[\"rewards\"]) > 50:\n",
    "            avg_rew = np.mean(history[\"rewards\"][-50:])\n",
    "            if avg_rew > best_avg_reward:\n",
    "                best_avg_reward = avg_rew\n",
    "                agent.save(\"best_model.pth\")\n",
    "                save_normalization_stats(env, \"best_model_stats.pkl\")\n",
    "                print(f\" >> New Best Model Saved! Reward: {best_avg_reward:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent, history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent, training_history = train()\n",
    "    plot_training_results(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, ClipAction, NormalizeObservation, TransformObservation, NormalizeReward, TransformReward\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import glob\n",
    "import base64\n",
    "import pickle\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    ENV_NAME = 'BipedalWalker-v3'\n",
    "    SEED = 9\n",
    "    \n",
    "    # Inference Settings\n",
    "    MODEL_PATH = \"./saved_models/best_model.pth\"\n",
    "    STATS_PATH = \"./saved_models/best_model_stats.pkl\" # Path to saved stats\n",
    "    VIDEO_DIR = \"./videos/inference\"\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. NETWORKS (Must Match Training Architecture)\n",
    "# ==========================================\n",
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(PolicyNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.LayerNorm(256),        \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),        \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_shape)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(output_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        raw_output = self.net(x)\n",
    "        mean = torch.tanh(raw_output) \n",
    "        return mean\n",
    "\n",
    "class CriticNN(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CriticNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "# ==========================================\n",
    "# 3. EVALUATION AGENT\n",
    "# ==========================================\n",
    "class EvalAgent:\n",
    "    def __init__(self, state_dim, act_dim):\n",
    "        self.policy = PolicyNN(state_dim, act_dim).to(Config.DEVICE)\n",
    "        self.critic = CriticNN(state_dim).to(Config.DEVICE)\n",
    "        self.policy.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "    def load(self, path):\n",
    "        if os.path.exists(path):\n",
    "            checkpoint = torch.load(path, map_location=Config.DEVICE)\n",
    "            self.policy.load_state_dict(checkpoint['policy'])\n",
    "            self.critic.load_state_dict(checkpoint['critic'])\n",
    "            print(f\"Successfully loaded model from {path}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model not found at {path}\")\n",
    "\n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            s = torch.FloatTensor(state).to(Config.DEVICE)\n",
    "            action = self.policy(s)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "# ==========================================\n",
    "# 4. EXECUTION\n",
    "# ==========================================\n",
    "def make_eval_env(video_folder, stats_path):\n",
    "    \"\"\"\n",
    "    Recreates the environment. If stats_path exists, loads the\n",
    "    running mean/var from training into NormalizeObservation.\n",
    "    \"\"\"\n",
    "    env = gym.make(Config.ENV_NAME, render_mode='rgb_array')\n",
    "    env = RecordVideo(env, video_folder, name_prefix=\"eval_run\", disable_logger=True)\n",
    "    env = ClipAction(env)\n",
    "    \n",
    "    # 1. Apply wrapper\n",
    "    env = NormalizeObservation(env) \n",
    "    \n",
    "    # 2. Load and inject statistics if they exist\n",
    "    if os.path.exists(stats_path):\n",
    "        with open(stats_path, 'rb') as f:\n",
    "            obs_rms = pickle.load(f)\n",
    "        \n",
    "        # Inject the loaded stats into the new environment\n",
    "        # We use .obs_rms to access the RunningMeanStd object\n",
    "        env.obs_rms = obs_rms\n",
    "        print(f\"Loaded normalization stats from {stats_path}\")\n",
    "    else:\n",
    "        print(f\"WARNING: No stats found at {stats_path}. Running with fresh normalization (performance may be poor).\")\n",
    "    \n",
    "    env = TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    return env\n",
    "\n",
    "def display_video(video_folder):\n",
    "    mp4_files = glob.glob(f'{video_folder}/*.mp4')\n",
    "    if mp4_files:\n",
    "        latest_file = max(mp4_files, key=os.path.getctime)\n",
    "        mp4 = open(latest_file, 'rb').read()\n",
    "        data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n",
    "        display(HTML(f\"<h3>Evaluation Run</h3><video width=600 controls><source src='{data_url}' type='video/mp4'></video>\"))\n",
    "    else:\n",
    "        print(\"No video file found.\")\n",
    "\n",
    "def run_inference():\n",
    "    # 1. Setup Environment to get dims\n",
    "    temp_env = gym.make(Config.ENV_NAME)\n",
    "    state_dim = temp_env.observation_space.shape[0]\n",
    "    act_dim = temp_env.action_space.shape[0]\n",
    "    temp_env.close()\n",
    "\n",
    "    # 2. Initialize Agent and Load Weights\n",
    "    agent = EvalAgent(state_dim, act_dim)\n",
    "    try:\n",
    "        agent.load(Config.MODEL_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # 3. Run Episode\n",
    "    env = make_eval_env(Config.VIDEO_DIR, Config.STATS_PATH)\n",
    "    state, _ = env.reset(seed=Config.SEED + 123) \n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    print(\"Running evaluation episode...\")\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, term, trunc, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = term or trunc\n",
    "        steps += 1\n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"Episode Finished.\")\n",
    "    print(f\"Total Reward: {total_reward:.2f}\")\n",
    "    print(f\"Total Steps: {steps}\")\n",
    "    \n",
    "    # 4. Show Video\n",
    "    display_video(Config.VIDEO_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
